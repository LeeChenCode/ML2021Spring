{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC5KwRyl6Flp"
      },
      "source": [
        "# Task description\n",
        "- Classify the speakers of given features.\n",
        "- Main goal: Learn how to use transformer.\n",
        "- Baselines:\n",
        "  - Easy: Run sample code and know how to use transformer.\n",
        "  - Medium: Know how to adjust parameters of transformer.\n",
        "  - Hard: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer. \n",
        "\n",
        "- Other links\n",
        "  - Kaggle: [link](https://www.kaggle.com/t/859c9ca9ede14fdea841be627c412322)\n",
        "  - Slide: [link](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW04/HW04.pdf)\n",
        "  - Data: [link](https://drive.google.com/file/d/1T0RPnu-Sg5eIPwQPfYysipfcz81MnsYe/view?usp=sharing)\n",
        "  - Video (Chinese): [link](https://www.youtube.com/watch?v=EPerg2UnGaI)\n",
        "  - Video (English): [link](https://www.youtube.com/watch?v=Gpz6AUvCak0)\n",
        "  - Solution for downloading dataset fail.: [link](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPDoreyypeJE"
      },
      "source": [
        "# Download dataset\n",
        "- Please follow [here](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e?usp=sharing) to download data\n",
        "- Data is [here](https://drive.google.com/file/d/1gaFy8RaQVUEXo2n0peCBR5gYKCB-mNHc/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QvpaILXnJIcw"
      },
      "outputs": [],
      "source": [
        "#!gdown --id 'paste your own data download link' --output Dataset.zip\n",
        "#!unzip Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Apr 11 22:44:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
            "| 25%   30C    P8     2W / 250W |     67MiB / 11018MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1313      G   /usr/lib/xorg/Xorg                 56MiB |\n",
            "|    0   N/A  N/A      1461      G   /usr/bin/gnome-shell                8MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1gYr_aoNDue"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz_NpuAipk3h"
      },
      "source": [
        "## Dataset\n",
        "- Original dataset is [Voxceleb1](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/).\n",
        "- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb1.\n",
        "- We randomly select 600 speakers from Voxceleb1.\n",
        "- Then preprocess the raw waveforms into mel-spectrograms.\n",
        "\n",
        "- Args:\n",
        "  - data_dir: The path to the data directory.\n",
        "  - metadata_path: The path to the metadata.\n",
        "  - segment_len: The length of audio segment for training. \n",
        "- The architecture of data directory \\\\\n",
        "  - data directory \\\\\n",
        "  |---- metadata.json \\\\\n",
        "  |---- testdata.json \\\\\n",
        "  |---- mapping.json \\\\\n",
        "  |---- uttr-{random string}.pt \\\\\n",
        "\n",
        "- The information in metadata\n",
        "  - \"n_mels\": The dimention of mel-spectrogram.\n",
        "  - \"speakers\": A dictionary. \n",
        "    - Key: speaker ids.\n",
        "    - value: \"feature_path\" and \"mel_len\"\n",
        "\n",
        "\n",
        "For efficiency, we segment the mel-spectrograms into segments in the traing step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cd7hoGhYtbXQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        " \n",
        "# default segment_len=128\n",
        "class myDataset(Dataset):\n",
        "  def __init__(self, data_dir, segment_len=128):\n",
        "    self.data_dir = data_dir\n",
        "    self.segment_len = segment_len\n",
        " \n",
        "    # Load the mapping from speaker neme to their corresponding id. \n",
        "    mapping_path = Path(data_dir) / \"mapping.json\"\n",
        "    mapping = json.load(mapping_path.open())\n",
        "    self.speaker2id = mapping[\"speaker2id\"]\n",
        " \n",
        "    # Load metadata of training data.\n",
        "    metadata_path = Path(data_dir) / \"metadata.json\"\n",
        "    metadata = json.load(open(metadata_path))[\"speakers\"]\n",
        " \n",
        "    # Get the total number of speaker.\n",
        "    self.speaker_num = len(metadata.keys())\n",
        "    self.data = []\n",
        "    for speaker in metadata.keys():\n",
        "      for utterances in metadata[speaker]:\n",
        "        self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        " \n",
        "  def __getitem__(self, index):\n",
        "    feat_path, speaker = self.data[index]\n",
        "    # Load preprocessed mel-spectrogram.\n",
        "    mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
        " \n",
        "    # Segmemt mel-spectrogram into \"segment_len\" frames.\n",
        "    if len(mel) > self.segment_len:\n",
        "      # Randomly get the starting point of the segment.\n",
        "      start = random.randint(0, len(mel) - self.segment_len)\n",
        "      # Get a segment with \"segment_len\" frames.\n",
        "      mel = torch.FloatTensor(mel[start:start+self.segment_len])\n",
        "    else:\n",
        "      mel = torch.FloatTensor(mel)\n",
        "    # Turn the speaker id into long for computing loss later.\n",
        "    speaker = torch.FloatTensor([speaker]).long()\n",
        "    return mel, speaker\n",
        " \n",
        "  def get_speaker_number(self):\n",
        "    return self.speaker_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqJxjoi_NGnB"
      },
      "source": [
        "## Dataloader\n",
        "- Split dataset into training dataset(90%) and validation dataset(10%).\n",
        "- Create dataloader to iterate the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zuT1AuFENI8t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "  # Process features within a batch.\n",
        "  #. zip(*) 可理解為解壓縮\n",
        "  \"\"\"Collate a batch of data.\"\"\"\n",
        "  mel, speaker = zip(*batch)\n",
        "  # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n",
        "  # 將mel對齊成一樣長度\n",
        "  mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n",
        "  # mel: (batch size, length, 40)\n",
        "  return mel, torch.FloatTensor(speaker).long()\n",
        "\n",
        "\n",
        "def get_dataloader(data_dir, batch_size, n_workers):\n",
        "  \"\"\"Generate dataloader\"\"\"\n",
        "  dataset = myDataset(data_dir)\n",
        "  speaker_num = dataset.get_speaker_number()\n",
        "  # Split dataset into training dataset and validation dataset\n",
        "  trainlen = int(0.9 * len(dataset))\n",
        "  lengths = [trainlen, len(dataset) - trainlen]\n",
        "  trainset, validset = random_split(dataset, lengths)\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "    trainset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=n_workers,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_batch,\n",
        "  )\n",
        "  valid_loader = DataLoader(\n",
        "    validset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=n_workers,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_batch,\n",
        "  )\n",
        "\n",
        "  return train_loader, valid_loader, speaker_num\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0x6eXiHpr4R"
      },
      "source": [
        "# Model\n",
        "- TransformerEncoderLayer:\n",
        "  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "  - Parameters:\n",
        "    - d_model: the number of expected features of the input (required).\n",
        "\n",
        "    - nhead: the number of heads of the multiheadattention models (required).\n",
        "\n",
        "    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "\n",
        "    - dropout: the dropout value (default=0.1).\n",
        "\n",
        "    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
        "\n",
        "- TransformerEncoder:\n",
        "  - TransformerEncoder is a stack of N transformer encoder layers\n",
        "  - Parameters:\n",
        "    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
        "\n",
        "    - num_layers: the number of sub-encoder-layers in the encoder (required).\n",
        "\n",
        "    - norm: the layer normalization component (optional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SHX4eVj4tjtd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# need to install conformer : pip install conformer\n",
        "from conformer import ConformerBlock\n",
        "\n",
        "\n",
        "# default : d_model=80, n_spks=600, dropout=0.1, dim_feedforward=256, nhead=2\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, d_model=256, n_spks=600, dropout=0.1):\n",
        "    super().__init__()\n",
        "    # Project the dimension of features from that of input into d_model.\n",
        "    self.prenet = nn.Linear(40, d_model)\n",
        "    # TODO:\n",
        "    #   Change Transformer to Conformer.\n",
        "    #   https://arxiv.org/abs/2005.08100\n",
        "    \"\"\"\"\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "      d_model=d_model, dim_feedforward=256, nhead=2, \n",
        "    )\n",
        "    # self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
        "    \"\"\"\n",
        "\n",
        "    # use conformer to replace transformer encoder\n",
        "    # inner dim of attention = dim_head * head \n",
        "    # ff_mult = expansion factor for inner linear layer of feedforward \n",
        "    # conv_expandsion factor : expansion factor for inner linear layer of convolution : dim in con = dim*conv_expandsion factor\n",
        "    self.conformer_block = ConformerBlock(\n",
        "      dim = d_model,\n",
        "      dim_head = 256,\n",
        "      heads = 1,\n",
        "      ff_mult = 4,\n",
        "      conv_expansion_factor = 18,\n",
        "      conv_kernel_size = 41,\n",
        "      attn_dropout = dropout,\n",
        "      ff_dropout = dropout,\n",
        "      conv_dropout = dropout\n",
        "    )\n",
        "    \n",
        "\n",
        "    # Project the the dimension of features from d_model into speaker nums.\n",
        "    self.pred_layer = nn.Sequential(\n",
        "      #nn.Linear(d_model, d_model),\n",
        "      nn.ReLU(),\n",
        "      #nn.BatchNorm1d(d_model),\n",
        "      #nn.Dropout(0.5),\n",
        "      nn.Linear(d_model, n_spks),\n",
        "    )\n",
        "\n",
        "  def forward(self, mels):\n",
        "    \"\"\"\n",
        "    args:\n",
        "      mels: (batch size, length, 40)\n",
        "    return:\n",
        "      out: (batch size, n_spks)\n",
        "    \"\"\"\n",
        "    # out: (batch size, length, d_model)\n",
        "    out = self.prenet(mels)\n",
        "    out = out.permute(1, 0, 2)\n",
        "\n",
        "    ''' transformer version\n",
        "    # out: (length, batch size, d_model)\n",
        "    # permute change dimension order \n",
        "    out = out.permute(1, 0, 2)\n",
        "    # The encoder layer expect features in the shape of (length, batch size, d_model) without batch first set to true.\n",
        "    out = self.encoder_layer(out)\n",
        "    out = out.transpose(0, 1)\n",
        "    #out: (batch size, length, d_model)\n",
        "\n",
        "    '''\n",
        "\n",
        "    out = self.conformer_block(out)\n",
        "    out = out.transpose(0, 1)\n",
        "\n",
        "    # mean pooling\n",
        "    stats = out.mean(dim=1)\n",
        "    # stats: (batch size, d_model)\n",
        "\n",
        "    # out: (batch, n_spks)\n",
        "    out = self.pred_layer(stats)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-__DolPGpvDZ"
      },
      "source": [
        "# Learning rate schedule\n",
        "- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n",
        "- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n",
        "- The warmup schedule\n",
        "  - Set learning rate to 0 in the beginning.\n",
        "  - The learning rate increases linearly from 0 to initial learning rate during warmup period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K-0816BntqT9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(\n",
        "  optimizer: Optimizer,\n",
        "  num_warmup_steps: int,\n",
        "  num_training_steps: int,\n",
        "  num_cycles: float = 0.5,\n",
        "  last_epoch: int = -1,\n",
        "):\n",
        "  \"\"\"\n",
        "  Create a schedule with a learning rate that decreases following the values of the cosine function between the\n",
        "  initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n",
        "  initial lr set in the optimizer.\n",
        "\n",
        "  Args:\n",
        "    optimizer (:class:`~torch.optim.Optimizer`):\n",
        "      The optimizer for which to schedule the learning rate.\n",
        "    num_warmup_steps (:obj:`int`):\n",
        "      The number of steps for the warmup phase.\n",
        "    num_training_steps (:obj:`int`):\n",
        "      The total number of training steps.\n",
        "    num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n",
        "      The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n",
        "      following a half-cosine).\n",
        "    last_epoch (:obj:`int`, `optional`, defaults to -1):\n",
        "      The index of the last epoch when resuming training.\n",
        "\n",
        "  Return:\n",
        "    :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
        "  \"\"\"\n",
        "\n",
        "  def lr_lambda(current_step):\n",
        "    # Warmup\n",
        "    if current_step < num_warmup_steps:\n",
        "      return float(current_step) / float(max(1, num_warmup_steps))\n",
        "    # decadence\n",
        "    progress = float(current_step - num_warmup_steps) / float(\n",
        "      max(1, num_training_steps - num_warmup_steps)\n",
        "    )\n",
        "    return max(\n",
        "      0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n",
        "    )\n",
        "\n",
        "  return LambdaLR(optimizer, lr_lambda, last_epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP03FFo9K8DS"
      },
      "source": [
        "# Model Function\n",
        "- Model forward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fohaLEFJK9-t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def model_fn(batch, model, criterion, device):\n",
        "  \"\"\"Forward a batch through the model.\"\"\"\n",
        "\n",
        "  mels, labels = batch\n",
        "  mels = mels.to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "  outs = model(mels)\n",
        "\n",
        "  loss = criterion(outs, labels)\n",
        "\n",
        "  # Get the speaker id with highest probability.\n",
        "  preds = outs.argmax(1)\n",
        "  # Compute accuracy.\n",
        "  accuracy = torch.mean((preds == labels).float())\n",
        "\n",
        "  return loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7cg-YrzLQcf"
      },
      "source": [
        "# Validate\n",
        "- Calculate accuracy of the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mD-_p6nWLO2L"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# record for visualization\n",
        "valid_loss_record = []\n",
        "valid_acc_record = []\n",
        "\n",
        "def valid(dataloader, model, criterion, device): \n",
        "  \"\"\"Validate on validation set.\"\"\"\n",
        "\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  running_accuracy = 0.0\n",
        "  pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n",
        "\n",
        "\n",
        "\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    with torch.no_grad():\n",
        "      loss, accuracy = model_fn(batch, model, criterion, device)\n",
        "      running_loss += loss.item()\n",
        "      running_accuracy += accuracy.item()\n",
        "\n",
        "    pbar.update(dataloader.batch_size)\n",
        "    pbar.set_postfix(\n",
        "      loss=f\"{running_loss / (i+1):.2f}\",\n",
        "      accuracy=f\"{running_accuracy / (i+1):.2f}\",\n",
        "    )\n",
        "\n",
        "  pbar.close()\n",
        "  model.train()\n",
        "\n",
        "  #record for visualization\n",
        "  valid_loss_record.append(running_loss / len(dataloader))\n",
        "  valid_acc_record.append(running_accuracy / len(dataloader))  \n",
        "\n",
        "  return running_accuracy / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHXyal5p1W5"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "chRQE7oYtw62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Info]: Use cuda now!\n",
            "[Info]: Finish loading data!\n",
            "[Info]: Finish creating model!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:31<00:00, 21.83 step/s, accuracy=0.09, loss=4.08, step=2000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1922.24 uttr/s, accuracy=0.21, loss=3.93]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.30 step/s, accuracy=0.44, loss=2.91, step=4000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1907.80 uttr/s, accuracy=0.36, loss=2.96]\n",
            "Train: 100% 2000/2000 [01:30<00:00, 21.99 step/s, accuracy=0.41, loss=2.67, step=6000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1924.55 uttr/s, accuracy=0.46, loss=2.47]\n",
            "Train: 100% 2000/2000 [01:34<00:00, 21.22 step/s, accuracy=0.50, loss=2.03, step=8000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1954.35 uttr/s, accuracy=0.53, loss=2.10]\n",
            "Train: 100% 2000/2000 [01:31<00:00, 21.96 step/s, accuracy=0.47, loss=1.84, step=1e+4]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1935.52 uttr/s, accuracy=0.57, loss=1.87]\n",
            "Train:   0% 4/2000 [00:00<02:44, 12.15 step/s, accuracy=0.62, loss=1.43, step=1e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 10000, best model saved. (accuracy=0.5701)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:34<00:00, 21.18 step/s, accuracy=0.62, loss=1.27, step=12000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1943.55 uttr/s, accuracy=0.63, loss=1.60]\n",
            "Train: 100% 2000/2000 [01:31<00:00, 21.84 step/s, accuracy=0.66, loss=1.27, step=14000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1914.63 uttr/s, accuracy=0.66, loss=1.46]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.33 step/s, accuracy=0.84, loss=0.89, step=16000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1956.14 uttr/s, accuracy=0.68, loss=1.37]\n",
            "Train: 100% 2000/2000 [01:31<00:00, 21.76 step/s, accuracy=0.78, loss=0.93, step=18000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1906.42 uttr/s, accuracy=0.70, loss=1.26]\n",
            "Train: 100% 2000/2000 [01:34<00:00, 21.22 step/s, accuracy=0.78, loss=1.30, step=2e+4] \n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1873.85 uttr/s, accuracy=0.72, loss=1.20]\n",
            "Train:   0% 4/2000 [00:00<03:05, 10.79 step/s, accuracy=0.88, loss=0.50, step=2e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20000, best model saved. (accuracy=0.7173)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:33<00:00, 21.47 step/s, accuracy=0.78, loss=0.83, step=22000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1958.55 uttr/s, accuracy=0.73, loss=1.14]\n",
            "Train: 100% 2000/2000 [01:34<00:00, 21.23 step/s, accuracy=0.91, loss=0.64, step=24000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1919.55 uttr/s, accuracy=0.75, loss=1.08]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.69 step/s, accuracy=0.94, loss=0.50, step=26000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1914.04 uttr/s, accuracy=0.77, loss=0.99]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.44 step/s, accuracy=0.88, loss=0.78, step=28000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1924.17 uttr/s, accuracy=0.77, loss=0.94]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.73 step/s, accuracy=0.84, loss=0.73, step=3e+4] \n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1880.73 uttr/s, accuracy=0.77, loss=0.96]\n",
            "Train:   0% 4/2000 [00:00<02:43, 12.21 step/s, accuracy=0.78, loss=1.09, step=3e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 30000, best model saved. (accuracy=0.7733)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:33<00:00, 21.42 step/s, accuracy=0.81, loss=0.72, step=32000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1941.46 uttr/s, accuracy=0.79, loss=0.90]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.68 step/s, accuracy=0.84, loss=0.63, step=34000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1878.35 uttr/s, accuracy=0.80, loss=0.86]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.50 step/s, accuracy=0.94, loss=0.34, step=36000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1961.14 uttr/s, accuracy=0.80, loss=0.86]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.51 step/s, accuracy=0.84, loss=0.43, step=38000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1917.24 uttr/s, accuracy=0.81, loss=0.82]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.53 step/s, accuracy=0.91, loss=0.35, step=4e+4] \n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1980.84 uttr/s, accuracy=0.81, loss=0.80]\n",
            "Train:   0% 4/2000 [00:00<02:44, 12.11 step/s, accuracy=0.97, loss=0.20, step=4e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40000, best model saved. (accuracy=0.8144)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:33<00:00, 21.41 step/s, accuracy=0.88, loss=0.33, step=42000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1948.19 uttr/s, accuracy=0.81, loss=0.84]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.60 step/s, accuracy=0.84, loss=0.53, step=44000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1917.82 uttr/s, accuracy=0.83, loss=0.74]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.51 step/s, accuracy=0.91, loss=0.23, step=46000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1960.11 uttr/s, accuracy=0.83, loss=0.75]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.62 step/s, accuracy=0.84, loss=0.45, step=48000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1919.88 uttr/s, accuracy=0.83, loss=0.74]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.42 step/s, accuracy=0.94, loss=0.16, step=5e+4] \n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1927.32 uttr/s, accuracy=0.84, loss=0.69]\n",
            "Train:   0% 4/2000 [00:00<02:44, 12.10 step/s, accuracy=0.97, loss=0.08, step=5e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 50000, best model saved. (accuracy=0.8426)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:31<00:00, 21.80 step/s, accuracy=0.91, loss=0.21, step=52000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1883.30 uttr/s, accuracy=0.85, loss=0.68]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.49 step/s, accuracy=1.00, loss=0.03, step=54000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1903.05 uttr/s, accuracy=0.85, loss=0.68]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.56 step/s, accuracy=0.97, loss=0.19, step=56000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1929.11 uttr/s, accuracy=0.85, loss=0.66]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.37 step/s, accuracy=0.94, loss=0.16, step=58000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1887.92 uttr/s, accuracy=0.86, loss=0.64]\n",
            "Train: 100% 2000/2000 [01:31<00:00, 21.78 step/s, accuracy=0.97, loss=0.16, step=6e+4] \n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1973.50 uttr/s, accuracy=0.86, loss=0.64]\n",
            "Train:   0% 4/2000 [00:00<03:10, 10.47 step/s, accuracy=0.97, loss=0.15, step=6e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60000, best model saved. (accuracy=0.8571)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:34<00:00, 21.17 step/s, accuracy=0.91, loss=0.14, step=62000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1968.51 uttr/s, accuracy=0.86, loss=0.61]\n",
            "Train: 100% 2000/2000 [01:31<00:00, 21.76 step/s, accuracy=0.94, loss=0.24, step=64000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1949.48 uttr/s, accuracy=0.86, loss=0.61]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.30 step/s, accuracy=1.00, loss=0.04, step=66000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1913.21 uttr/s, accuracy=0.87, loss=0.59]\n",
            "Train: 100% 2000/2000 [01:31<00:00, 21.79 step/s, accuracy=0.97, loss=0.12, step=68000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1960.05 uttr/s, accuracy=0.87, loss=0.61]\n",
            "Train: 100% 2000/2000 [01:35<00:00, 21.03 step/s, accuracy=0.94, loss=0.25, step=7e+4] \n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1870.13 uttr/s, accuracy=0.87, loss=0.57]\n",
            "Train:   0% 4/2000 [00:00<03:04, 10.79 step/s, accuracy=0.94, loss=0.21, step=7e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 70000, best model saved. (accuracy=0.8746)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:31<00:00, 21.95 step/s, accuracy=1.00, loss=0.05, step=72000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 2034.87 uttr/s, accuracy=0.87, loss=0.61]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.28 step/s, accuracy=1.00, loss=0.04, step=74000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1879.40 uttr/s, accuracy=0.88, loss=0.56]\n",
            "Train: 100% 2000/2000 [01:30<00:00, 22.01 step/s, accuracy=0.97, loss=0.11, step=76000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1967.81 uttr/s, accuracy=0.87, loss=0.58]\n",
            "Train: 100% 2000/2000 [01:34<00:00, 21.09 step/s, accuracy=0.97, loss=0.18, step=78000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1935.03 uttr/s, accuracy=0.88, loss=0.54]\n",
            "Train: 100% 2000/2000 [01:30<00:00, 22.07 step/s, accuracy=1.00, loss=0.04, step=8e+4] \n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1925.82 uttr/s, accuracy=0.88, loss=0.54]\n",
            "Train:   0% 4/2000 [00:00<03:15, 10.20 step/s, accuracy=0.97, loss=0.09, step=8e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 80000, best model saved. (accuracy=0.8821)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:36<00:00, 20.76 step/s, accuracy=0.94, loss=0.12, step=82000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1930.75 uttr/s, accuracy=0.88, loss=0.55]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.69 step/s, accuracy=1.00, loss=0.02, step=84000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1855.90 uttr/s, accuracy=0.89, loss=0.51]\n",
            "Train: 100% 2000/2000 [01:34<00:00, 21.07 step/s, accuracy=1.00, loss=0.16, step=86000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1921.13 uttr/s, accuracy=0.89, loss=0.53]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.73 step/s, accuracy=1.00, loss=0.01, step=88000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1889.17 uttr/s, accuracy=0.88, loss=0.52]\n",
            "Train: 100% 2000/2000 [01:35<00:00, 20.91 step/s, accuracy=0.97, loss=0.06, step=9e+4] \n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1948.39 uttr/s, accuracy=0.89, loss=0.52]\n",
            "Train:   0% 4/2000 [00:00<02:44, 12.15 step/s, accuracy=1.00, loss=0.06, step=9e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 90000, best model saved. (accuracy=0.8903)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 2000/2000 [01:31<00:00, 21.76 step/s, accuracy=0.97, loss=0.06, step=92000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1906.41 uttr/s, accuracy=0.89, loss=0.50]\n",
            "Train: 100% 2000/2000 [01:34<00:00, 21.07 step/s, accuracy=1.00, loss=0.04, step=94000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1921.67 uttr/s, accuracy=0.89, loss=0.51]\n",
            "Train: 100% 2000/2000 [01:32<00:00, 21.60 step/s, accuracy=1.00, loss=0.06, step=96000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1891.92 uttr/s, accuracy=0.89, loss=0.51]\n",
            "Train: 100% 2000/2000 [01:33<00:00, 21.29 step/s, accuracy=1.00, loss=0.01, step=98000]\n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1901.89 uttr/s, accuracy=0.89, loss=0.50]\n",
            "Train: 100% 2000/2000 [01:31<00:00, 21.92 step/s, accuracy=0.94, loss=0.20, step=1e+5] \n",
            "Valid: 100% 6944/6944 [00:03<00:00, 1872.07 uttr/s, accuracy=0.89, loss=0.51]\n",
            "Train:   0% 0/2000 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100000, best model saved. (accuracy=0.8903)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "#record for visualization\n",
        "train_loss_record = []\n",
        "train_acc_record = []\n",
        "\n",
        "def parse_args():\n",
        "  \"\"\"arguments\"\"\"\n",
        "  config = {\n",
        "    \"data_dir\": \"./Dataset\",\n",
        "    \"save_path\": \"model.ckpt\",\n",
        "    \"batch_size\": 32,\n",
        "    \"n_workers\": 24,\n",
        "    \"valid_steps\": 2000,\n",
        "    \"warmup_steps\": 1000,\n",
        "    \"save_steps\": 10000,\n",
        "    \"total_steps\": 100000, #default : 70000\n",
        "  }\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def main(\n",
        "  data_dir,\n",
        "  save_path,\n",
        "  batch_size,\n",
        "  n_workers,\n",
        "  valid_steps,\n",
        "  warmup_steps,\n",
        "  total_steps,\n",
        "  save_steps,\n",
        "):\n",
        "  \"\"\"Main function.\"\"\"\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"[Info]: Use {device} now!\")\n",
        "\n",
        "  train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n",
        "  train_iterator = iter(train_loader)\n",
        "  print(f\"[Info]: Finish loading data!\",flush = True)\n",
        "\n",
        "  model = Classifier(n_spks=speaker_num).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = AdamW(model.parameters(), lr=1e-3)\n",
        "  scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "  print(f\"[Info]: Finish creating model!\",flush = True)\n",
        "\n",
        "  best_accuracy = -1.0\n",
        "  best_state_dict = None\n",
        "\n",
        "\n",
        "  pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
        "\n",
        "  for step in range(total_steps):\n",
        "    # Get data\n",
        "    try:\n",
        "      batch = next(train_iterator)\n",
        "    except StopIteration:\n",
        "      train_iterator = iter(train_loader)\n",
        "      batch = next(train_iterator)\n",
        "\n",
        "    loss, accuracy = model_fn(batch, model, criterion, device)\n",
        "    batch_loss = loss.item()\n",
        "    batch_accuracy = accuracy.item()\n",
        "    \n",
        "    train_loss_record.append(batch_loss)\n",
        "    train_acc_record.append(batch_accuracy)\n",
        "    \n",
        "    # Updata model\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Log\n",
        "    pbar.update()\n",
        "    pbar.set_postfix(\n",
        "      loss=f\"{batch_loss:.2f}\",\n",
        "      accuracy=f\"{batch_accuracy:.2f}\",\n",
        "      step=step + 1,\n",
        "    )\n",
        "\n",
        "    # Do validation every valid_step\n",
        "    if (step + 1) % valid_steps == 0:\n",
        "      pbar.close()\n",
        "\n",
        "      valid_accuracy = valid(valid_loader, model, criterion, device)\n",
        "\n",
        "      # keep the best model\n",
        "      if valid_accuracy > best_accuracy:\n",
        "        best_accuracy = valid_accuracy\n",
        "        best_state_dict = model.state_dict()\n",
        "\n",
        "      pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
        "\n",
        "    # Save the best model so far.\n",
        "    if (step + 1) % save_steps == 0 and best_state_dict is not None:\n",
        "      torch.save(best_state_dict, save_path)\n",
        "      pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n",
        "\n",
        "  pbar.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(**parse_args())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R2rx3AyHpQ-"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSuI3WY9Fz78"
      },
      "source": [
        "## Dataset of inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4evns0055Dsx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "  def __init__(self, data_dir):\n",
        "    testdata_path = Path(data_dir) / \"testdata.json\"\n",
        "    metadata = json.load(testdata_path.open())\n",
        "    self.data_dir = data_dir\n",
        "    self.data = metadata[\"utterances\"]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    utterance = self.data[index]\n",
        "    feat_path = utterance[\"feature_path\"]\n",
        "    mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
        "\n",
        "    return feat_path, mel\n",
        "\n",
        "\n",
        "def inference_collate_batch(batch):\n",
        "  \"\"\"Collate a batch of data.\"\"\"\n",
        "  feat_paths, mels = zip(*batch)\n",
        "\n",
        "  return feat_paths, torch.stack(mels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAinHBG1GIWv"
      },
      "source": [
        "## Main funcrion of Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yQaTt7VDHoRI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Info]: Use cuda now!\n",
            "[Info]: Finish loading data!\n",
            "[Info]: Finish creating model!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [02:27<00:00, 40.61it/s]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "from pathlib import Path\n",
        "#from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm as tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def parse_args():\n",
        "  \"\"\"arguments\"\"\"\n",
        "  config = {\n",
        "    \"data_dir\": \"./Dataset\",\n",
        "    \"model_path\": \"./model.ckpt\",\n",
        "    \"output_path\": \"./output.csv\",\n",
        "  }\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def main(\n",
        "  data_dir,\n",
        "  model_path,\n",
        "  output_path,\n",
        "):\n",
        "  \"\"\"Main function.\"\"\"\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"[Info]: Use {device} now!\")\n",
        "\n",
        "  mapping_path = Path(data_dir) / \"mapping.json\"\n",
        "  mapping = json.load(mapping_path.open())\n",
        "\n",
        "  dataset = InferenceDataset(data_dir)\n",
        "  dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=8,\n",
        "    collate_fn=inference_collate_batch,\n",
        "  )\n",
        "  print(f\"[Info]: Finish loading data!\",flush = True)\n",
        "\n",
        "  speaker_num = len(mapping[\"id2speaker\"])\n",
        "  model = Classifier(n_spks=speaker_num).to(device)\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  model.eval()\n",
        "  print(f\"[Info]: Finish creating model!\",flush = True)\n",
        "\n",
        "  results = [[\"Id\", \"Category\"]]\n",
        "  for feat_paths, mels in tqdm(dataloader):\n",
        "    with torch.no_grad():\n",
        "      mels = mels.to(device)\n",
        "      outs = model(mels)\n",
        "      preds = outs.argmax(1).cpu().numpy()\n",
        "      for feat_path, pred in zip(feat_paths, preds):\n",
        "        results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n",
        "  \n",
        "  with open(output_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(**parse_args())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Visualize Result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.arange(len(train_acc_record))\n",
        "train_acc = torch.tensor(train_acc_record, device='cpu')\n",
        "train_loss = torch.tensor(train_loss_record, device='cpu')\n",
        "\n",
        "plt.title(\"Learning curve of training\")\n",
        "plt.xlabel(\"steps\")\n",
        "plt.plot(x, train_acc, color=\"blue\", label=\"Train Acc\")\n",
        "plt.plot(x, train_loss, color=\"red\", label=\"Train Loss\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAtDklEQVR4nO3dd5xU1f3/8deH3YWlSReRpdmigjQXYgFEjTVGNBqVfKNiEts3dlM0RYxf/ZoYE/s3xm9iy1cpiQaxR6NR8KdIcUEBK4I0pXcX2N3P748z4w7DLDvszu7dmXk/H4/7mHLP3Pu5szufOXPuueeYuyMiItmvWdQBiIhIZiihi4jkCCV0EZEcoYQuIpIjlNBFRHKEErqISI5QQpeMMbPhZvZB1HE0FWZ2pJl9ZGabzOy0DG53pJktSXg818xGplO2Dvu638x+VdfXS+NSQs8RZrbQzL4RZQzuPsXdvxZlDE3MTcC97t7G3Sc11E7cva+7/7u+2zGzMWY2NWnbl7j7f9V329I4lNAlbWZWEHUM9dXIx9ALmNuI+5M8p4Se48ysmZldZ2afmNlqM5toZh0T1v/NzD43s/Vm9rqZ9U1Y97CZ/dHMnjOzzcDRsV8CPzazObHXTDCz4lj55KaAGsvG1v/UzJab2TIz+6GZuZntV8NxdDSzh2Jl15rZpNjzO9UqE7eT4hh+HDvegoTyp5vZnHTerxRxXWhmH5vZGjObbGZ7x57/BNgHeDrW5NIi6XU/M7O/Jz13l5ndHbt/gZnNN7ONZrbAzC7eRQxf/Tozs5axY15rZvOAIUll48e20czmmdnpsecPAu4HDo/Fuy7h/bu5tuNNeN8viTUzrTOz+8zMaopbMk8JPfddDpwGHAXsDawF7ktY/zywP7AnMAt4LOn13wVuAdoC8cR5FnAi0AfoD4zZxf5TljWzE4FrgG8A+wEjazmOvwKtgL6xWO+opXxNx3AXsBk4Jmn947H7tb1fXzGzY4BbCcfYDVgEjAdw932Bz4BvxZpctia9fDxwspm1jW2rILadeBwrgFOAPYALgDvMbHAaxzoW2De2nACcn7T+E2A40A74NfB/ZtbN3ecDlwBvxuJtvzvHm+AUwpdI/1i5E9KIWTLF3bXkwAIsBL6R4vn5wLEJj7sB24HCFGXbAw60iz1+GHg0xX6+l/D4NuD+2P2RwJI0yz4I3Jqwbr/YvvdLEVc3oArokGLdGGBq0nNfbaeGY7gZeDB2vy0hwfeqw/v1F+C2hMdtYmV77+pvklB+KnBe7P5xwCe7KDsJuHIX7/M3YvcXACcmrLsosWyK7ZYBo3bxXj4M3Jzm8TowLGH9ROC6qD8b+bSohp77egH/iP0EXkdIWJVAVzMrMLPfxH6CbyAkBoDOCa9fnGKbnyfc30L4YNekprJ7J2071X7iegBr3H3tLsrsSvK2Hwe+HWsG+TYwy90XxdbV+H6l2O7ehFoqAO6+CVgNdE8zrseB0bH7ib8SMLOTzOytWNPGOuBkdvy71CT5fV2UuNLMzjOzsoTj65fmduPbru14d+d/QzJMCT33LQZOcvf2CUuxuy8lJJFRhGaPdkDv2GsS2z0bajjO5UBJwuMeuyi7GOhoZu1TrNtMaIoBwMz2SlFmh2Nw93mExHQSSYmUXb9fyZYRvgDi+24NdAJSlU3lb8BIMysBTo/HEfuieQK4HejqofnjOXb8u9RkOTu+lz0T4usF/C9wGdAptt33ErZb29+6vscrDUwJPbcUmVlxwlJIONF1S+zDjJl1MbNRsfJtga2EWlYr4L8bMdaJwAVmdpCZtQJq7Ovs7ssJbf3/Y2YdzKzIzEbEVs8G+prZwNgJ1xvT3P/jwJXACEJijdvV+5VsXOwYBsaS8H8D09x9YToBuPtK4N/AQ8CnHtqxAZoDLYCVQIWZnQQcn+ZxTQSuj71PJYRzAnGtCUl7ZezYLiDU0OO+AErMrHkN267X8UrDU0LPLc8BXyYsNxJOAk4G/mlmG4G3gK/Hyj9KqKkuBebF1jUKd38euBt4Ffg4Yd/JJw/jziW0175POGF4VWw7HxL6e78MfET1idvajCOc+HzF3VclPL+r9yv5GF4mfBE9QagZ7wuck+b+4x4n/EL66leCu28EriAk57WEXxGT09zerwl/00+BfxJOJse3Ow/4PfAmIXkfAryR8NpXCN0sPzezxPck/vpMHK80IIudvBCJVKzb3HtAC3eviDoekWykGrpEJtb/u4WZdQB+CzytZC5Sd0roEqWLCc0nnxB6klwabTgi2U1NLiIiOUI1dBGRHFEY1Y47d+7svXv3jmr3IiJZaebMmavcvUuqdZEl9N69ezNjxoyodi8ikpXMbFFN69TkIiKSI9JO6LFxP94xs2dSrGthYWjUj81smpn1zmiUIiJSq92poV9JGKgolR8Aa919P8Kwpr+tb2AiIrJ70mpDj40J8U3CmNLXpCgyiuoxNP4O3Gtm5uoTKZI3tm/fzpIlSygvL486lJxQXFxMSUkJRUVFab8m3ZOidwI/JQzmlEp3YkN2unuFma0njMK2w3gQZnYRYXxmevbsmbwNEcliS5YsoW3btvTu3RtNVFQ/7s7q1atZsmQJffr0Sft1tTa5mNkpwAp3n1mfAAHc/QF3L3X30i5dUva6EZEsVV5eTqdOnZTMM8DM6NSp027/2kmnDf1I4FQzW0iYbuoYM/u/pDJLiY3BHBuytR1hSFYRySNK5plTl/ey1oTu7te7e4m79yYMlfmKu38vqdhkqucuPDNWpmHaz999F37+c1hb18lrRERyU537oZvZTWZ2auzhX4BOZvYx4aTpdZkILqVPPoFbb4WPP26wXYhI9jn66KN58cUXd3juzjvv5NJLax7zbeTIkV9d4HjyySezbt26ncrceOON3H777TVuY+DAgZxzTtMYFn63Erq7/9vdT4ndv8HdJ8ful7v7d9x9P3cf6u4LGiJYAHrFZsBaVOPFUiKSh0aPHs348eN3eG78+PGMHj26hlfs6LnnnqN9+/a7tc/58+dTWVnJlClT2Lx58269tiFk35WiSugiksKZZ57Js88+y7Zt2wBYuHAhy5YtY/jw4Vx66aWUlpbSt29fxo4dm/L1vXv3ZtWq0DHvlltu4YADDmDYsGF88MEHNe5z3LhxnHvuuRx//PE89dRTXz0/ffp0jjjiCAYMGMDQoUPZuHEjlZWV/PjHP6Zfv37079+fe+65J4NHH0Q2lkuddegAbdoooYs0YVddBWVlmd3mwIFw5501r+/YsSNDhw7l+eefZ9SoUYwfP56zzjoLM+OWW26hY8eOVFZWcuyxxzJnzhz69++fcjszZ85k/PjxlJWVUVFRweDBgzn00ENTlp0wYQIvvfQS77//Pvfccw/f/e532bZtG2effTYTJkxgyJAhbNiwgZYtW/LAAw+wcOFCysrKKCwsZM2aNfV/U5JkXw3dLNTSldBFJElis0tic8vEiRMZPHgwgwYNYu7cucybN6/GbUyZMoXTTz+dVq1asccee3DqqaemLDdjxgw6d+5Mz549OfbYY3nnnXdYs2YNH3zwAd26dWPIkCEA7LHHHhQWFvLyyy9z8cUXU1gY6tEdO3bM5KED2VhDByV0kSZuVzXphjRq1CiuvvpqZs2axZYtWzj00EP59NNPuf3225k+fTodOnRgzJgxGbmaddy4cbz//vvEhwHfsGEDTzzxBIcddli9t11X2VdDByV0EUmpTZs2HH300Xz/+9//qna+YcMGWrduTbt27fjiiy94/vnnd7mNESNGMGnSJL788ks2btzI008/vVOZqqoqJk6cyLvvvsvChQtZuHAhTz31FOPGjeNrX/say5cvZ/r06QBs3LiRiooKjjvuOP70pz9RURGmzW2IJpfsraGvWwcbNsAee0QdjYg0IaNHj+b000//qullwIABDBo0iAMPPJAePXpw5JFH7vL1gwcP5uyzz2bAgAHsueeeXzWdJJoyZQrdu3dn7733/uq5ESNGMG/ePFavXs2ECRO4/PLL+fLLL2nZsiUvv/wyP/zhD/nwww/p378/RUVFXHjhhVx22WUZPfbI5hQtLS31Ok9wMX48jB4dLjLq1y+zgYlIncyfP5+DDjoo6jBySqr31MxmuntpqvLZ2+QCanYREUmghC4ikiOyM6HvtRc0b66ELiKSIDsTerNm0KOHErqISILsTOigrosiIkmU0EVEckR2J/Tly2Hr1qgjEZEmoLGHz61tWN0oZHdCB1i8ONo4RKRJiGL43KYm+xO6ml1EhGiGz03m7vzkJz+hX79+HHLIIUyYMAGA5cuXM2LECAYOHEi/fv2YMmUKlZWVjBkz5quyd9xxRz3fgWy99B+U0EWasgjGz41i+NxkTz75JGVlZcyePZtVq1YxZMgQRowYweOPP84JJ5zAL37xCyorK9myZQtlZWUsXbqU9957DyBlc8/uyt4aeklJGEpXCV1EYhpz+NxUpk6dyujRoykoKKBr164cddRRTJ8+nSFDhvDQQw9x44038u6779K2bVv22WcfFixYwOWXX84LL7zAHhkYl6rWGrqZFQOvAy1i5f/u7mOTyowBfgcsjT11r7v/ud7R7Urz5rD33kroIk1RROPnNubwubtjxIgRvP766zz77LOMGTOGa665hvPOO4/Zs2fz4osvcv/99zNx4kQefPDBeu0nnRr6VuAYdx8ADARONLNUA/5OcPeBsaVhk3mcui6KSILGGj63JsOHD2fChAlUVlaycuVKXn/9dYYOHcqiRYvo2rUrF154IT/84Q+ZNWsWq1atoqqqijPOOIObb76ZWbNm1evYIY0auofhGDfFHhbFlmiGaEzWqxe89VbUUYhIE9IYw+fG3XzzzdyZ8Gtk8eLFvPnmmwwYMAAz47bbbmOvvfbikUce4Xe/+x1FRUW0adOGRx99lKVLl3LBBRdQVVUFwK233lrvY09r+FwzKwBmAvsB97n7z5LWjwFuBVYCHwJXu/tO/QnN7CLgIoCePXseuqi+tevrr4fbb4fycigoqN+2RKReNHxu5jXI8LnuXunuA4ESYKiZJQ9C/jTQ2937Ay8Bj9SwnQfcvdTdS7t06ZLOrnetVy+oqAgXGImI5Lnd6uXi7uuAV4ETk55f7e7xSzb/DKTXx6e+1HVRROQrtSZ0M+tiZu1j91sCxwHvJ5XplvDwVGB+BmOsmRK6SJMS1Qxouagu72U6FxZ1Ax6JtaM3Aya6+zNmdhMww90nA1eY2alABbAGGLPbkdSFErpIk1FcXMzq1avp1KkTZhZ1OFnN3Vm9ejXFxcW79bp0ernMAQaleP6GhPvXA9fv1p4zoXVr6NRJCV2kCSgpKWHJkiWsXLky6lByQnFxMSUlJbv1muy99D9OfdFFmoSioiL69OkTdRh5LXsv/Y9TQhcRAXIpoetkjIjkudxI6Fu2wOrVUUciIhKp7E/oPXuGWzW7iEiey/6Erq6LIiKAErqISM7I/oTeqRO0agWffRZ1JCIikcr+hG6mrosiIuRCQgcldBERlNBFRHJG7iT01ath8+aoIxERiUzuJHRQLV1E8poSuohIjlBCFxHJEbmR0Lt1g8JCJXQRyWu5kdALCqBHDyV0Eclr6cwpWmxmb5vZbDOba2a/TlGmhZlNMLOPzWyamfVukGh3RV0XRSTPpVND3woc4+4DgIHAiWZ2WFKZHwBr3X0/4A7gtxmNMh1K6CKS52pN6B5sij0sii3Js0mMAh6J3f87cKw19iyxvXrBsmWwbVuj7lZEpKlIqw3dzArMrAxYAbzk7tOSinQHFgO4ewWwHuiUwThr16tXmLVoyZJG3a2ISFORVkJ390p3HwiUAEPNrF9ddmZmF5nZDDObkfGZwdV1UUTy3G71cnH3dcCrwIlJq5YCPQDMrBBoB+w0J5y7P+Dupe5e2qVLlzoFXKPevcPtJ59kdrsiIlkinV4uXcysfex+S+A44P2kYpOB82P3zwRecW/kWZv79IEOHeDNNxt1tyIiTUVhGmW6AY+YWQHhC2Ciuz9jZjcBM9x9MvAX4K9m9jGwBjinwSKuSbNmcOSRMHVqo+9aRKQpqDWhu/scYFCK529IuF8OfCezodXB8OHwzDOwYgXsuWfU0YiINKrcuFI0btiwcPvGG9HGISISgdxK6IceCi1aqNlFRPJSbiX0Fi1g6FAldBHJS7mV0CE0u8yapdmLRCTv5F5CHz4cKipgWvLFrCIiuS33Evrhh4OZml1EJO/kXkJv3x4OOUQJXUTyTu4ldAjt6G++GZpeRETyRO4m9E2bYM6cqCMREWk0uZnQhw8Pt1OmRBuHiEgjys2EXlIShtNVO7qI5JHcTOgQml2mTg2TXoiI5IHcTuiffw4LFkQdiYhIo8jthA5qdhGRvJG7Cf3gg8OEF0roIpIncjehxye8UE8XEckTuZvQITS7fPABZHpCahGRJij3EzpowgsRyQvpTBLdw8xeNbN5ZjbXzK5MUWakma03s7LYckOqbTW60lJNeCEieSOdSaIrgGvdfZaZtQVmmtlL7j4vqdwUdz8l8yHWgya8EJE8UmsN3d2Xu/us2P2NwHyge0MHljHDhsHMmbBlS9SRiIg0qN1qQzez3sAgINXsEYeb2Wwze97M+tbw+ovMbIaZzVjZWCcqhw3ThBcikhfSTuhm1gZ4ArjK3TckrZ4F9HL3AcA9wKRU23D3B9y91N1Lu3TpUseQd9MRR4QJL9R9UURyXFoJ3cyKCMn8MXd/Mnm9u29w902x+88BRWbWOaOR1lX79nDoofDss1FHIiLSoNLp5WLAX4D57v6HGsrsFSuHmQ2NbXd1JgOtl29/G95+GxYvjjoSEZEGk04N/UjgXOCYhG6JJ5vZJWZ2SazMmcB7ZjYbuBs4x70JDXN4xhnh9h//iDYOEZEGZFHl3dLSUp8xY0bj7bBfP+jcGf7978bbp4hIhpnZTHcvTbUut68UTXTGGeHE6IoVUUciItIg8iuhV1XBpElRRyIi0iDyJ6Efcgjsuy88uVMnHRGRnJA/Cd0s1NL/9S9YuzbqaEREMi5/EjqE7osVFfDMM1FHIiKScfmV0IcMgZISeOKJqCMREcm4/ErozZrB6afDiy/Cpk1RRyMiklH5ldAhtKOXl8Pzz0cdiYhIRuVfQh82DLp0UbOLiOSc/EvoBQVw2mlhsK7y8qijERHJmPxL6BCaXTZtgpdeijoSEZGMyc+EfvTR0K6dLjISkZySnwm9eXM49VR46inYvj3qaEREMiI/EzqEi4zWrtXoiyKSM/I3oZ9wArRurd4uIpIz8jeht2wZml3Gj4d166KORkSk3vI3oQP87Gewfj3ceWfUkYiI1Ft+J/QBA0Kf9DvvVC1dRLJeOpNE9zCzV81snpnNNbMrU5QxM7vbzD42szlmNrhhwm0AN9wQaul33RV1JCIi9ZJODb0CuNbdDwYOA35kZgcnlTkJ2D+2XAT8MaNRNqRBg2DUqFBLX78+6mhEROqs1oTu7svdfVbs/kZgPtA9qdgo4FEP3gLam1m3jEfbUG64ITS53H131JGIiNTZbrWhm1lvYBAwLWlVd2BxwuMl7Jz0MbOLzGyGmc1YuXLlbobagAYPDj1e/vAH1dJFJGulndDNrA3wBHCVu2+oy87c/QF3L3X30i5dutRlEw0nXku/556oIxERqZO0ErqZFRGS+WPunmoAlKVAj4THJbHnssehh8Ipp4Ra+oY6fV+JiEQqnV4uBvwFmO/uf6ih2GTgvFhvl8OA9e6+PINxNo6xY8NwAKqli0gWSqeGfiRwLnCMmZXFlpPN7BIzuyRW5jlgAfAx8L/AfzZMuA2stBS++c1QS9+4MepoRER2S2FtBdx9KmC1lHHgR5kKKlJjx8LQoXDvvXD99VFHIyKStvy+UjSVIUPg5JPh9tth8+aooxERSZsSeirXXQdr1sBjj0UdiYhI2pTQUxk2DAYODBcauUcdjYhIWpTQUzGDK6+EuXPhlVeijkZEJC1K6DU55xzo3FnDAYhI1lBCr0lxMVx8MTz9NCxYEHU0IiK1UkLflUsvhYICuO++qCMREamVEvqudO8OZ54Jf/kLbNoUdTQiIrukhF6bK64IIzA++mjUkYiI7JISem0OOywMCXDPPVBVFXU0IiI1UkKvTbwL4/vvw0svRR2NiEiNlNDT8Z3vQNeu6sIoIk2aEno6WrSASy6B556Djz6KOhoRkZSU0NN1ySVQVKSx0kWkyVJCT9dee8HZZ8NDD4WBu0REmhgl9N1x7bVQXh76pm/bFnU0IiI7UELfHQMHwoMPwquvwve/r5EYRaRJSWdO0QfNbIWZvVfD+pFmtj5herobMh9mE3LuuXDLLWGs9F/8IupoRES+UusUdMDDwL3Ari6VnOLup2Qkomxw/fWwaBHceiv07BlOmIqIRCydOUVfN7PejRBL9jALA3YtXQo/+lEY8+Vb34o6KhHJc5lqQz/czGab2fNm1remQmZ2kZnNMLMZK1euzNCuI1JYCBMmwODBoffL229HHZGI5LlMJPRZQC93HwDcA0yqqaC7P+Dupe5e2qVLlwzsOmKtW8Mzz4QujaecEoYHEBGJSL0TurtvcPdNsfvPAUVm1rnekWWLrl3hhRdCj5fDD4d//jPqiEQkT9U7oZvZXmZmsftDY9tcXd/tZpUDDghNLj16wEknwR13qEujiDS6Wk+Kmtk4YCTQ2cyWAGOBIgB3vx84E7jUzCqAL4Fz3PMwm/XpA//v/8F558E118CcOXD//WEcGBGRRmBR5d7S0lKfMWNGJPtuUFVVcNNN8Otfh7HUn3wSunWLOioRyRFmNtPdS1Ot05WimdasGdx4I/ztb6GWPmQIzJwZdVQikgeU0BvKmWfCG2+ESaaPOgpefDHqiEQkxymhN6SBA+Gtt2D//UO3xr/+NeqIRCSHKaE3tG7d4LXXYMSIcML0d79TDxgRaRBK6I1hjz3CbEdnnw0//WkYhlcTTotIhqUzOJdkQosW8Pjj4arSO+6A5cvh4YfVrVFEMkY19MbUrFlI5r/9LYwfDyNHhhOnIiIZoITe2MxCs8u4cfDppzBsWLi6NBf75ItIo1JCj8o558CCBXDbbTB9euivftppoe+6iEgdKKFHqVUr+MlPQk39v/4L/v1vGDAgnDxdsCDq6EQkyyihNwVt28IvfxkS+y9/GYbkPeigMDPSxo1RRyciWUIJvSnp0CHU1D/8MDTJ/OY34aKkBx+EysqooxORJk4JvSnq3h0eeQSmTYN99oEf/CC0sb/+etSRiUgTpoTelA0dGro1Pv44rFwZxoT51rdg9uyoIxORJkgJvakzg9Gj4YMP4JZbYOrUMEbMOeeE50REYpTQs0WrVvDzn4feLz//eThxevDB8P3vw6JFUUcnIk2AEnq26dAh1NQXLIArrgjNMfvvDzffrPFhRPKcEnq22nPPMIzARx/BGWfAr34FJ58c2tpFJC/VmtDN7EEzW2Fm79Ww3szsbjP72MzmmNngzIcpNerRI9TS//SncGHSoEFhblMRyTvp1NAfBk7cxfqTgP1jy0XAH+sfluwWM7joopDIW7QIvWH+8AeNuy6SZ2pN6O7+OrBmF0VGAY968BbQ3sw0K3IUBg+GWbNC18Zrr4VvfxvWro06KhFpJJloQ+8OLE54vCT23E7M7CIzm2FmM1aqrbdhtGsHTzwRaujPPAM9e4aeMK+/rhq7SI5r1Aku3P0B4AGA0tJSZZeGYgZXXw1HHw333QcTJsBDD4WrTs8/P0yF17s3bN8eujx+8kn1snlzaL4ZrFMhItnGPI1am5n1Bp5x934p1v0J+Le7j4s9/gAY6e7Ld7XN0tJSn6ExwBvHli3w5JNhhqRXXgk19ZISWLZsx66OLVuGSTg2bw7NNmPHwqGHRha2iOzMzGa6e2mqdZlocpkMnBfr7XIYsL62ZC6NrFUr+N734OWXq4fqPeqocIHSQw+F5pilS0MiX7o0rJ86FUpLQ2LXF69IVqi1hm5m44CRQGfgC2AsUATg7vebmQH3EnrCbAEucPdaM4Bq6E3chg1wzz3w+9+HE6vf/CZccAEcf3wY7lcki1VUhB+nzZqFFsr4bSL36iVetqCgfvt1D/sGKCqq2zZ2VUNPq8mlISihZ4kNG+Dee8NFTKtWQfPmoW3+1FND7b1Hj6gjlN3kHn6MrVkD69aFxNKmTVhatw5/4rjy8vB9vmZNuF27Npx6adUqtNDFb1u2DMluwwZYv7562bAhDOm/dWt43bZt4TZ+H6qTaXxp1izsd8OGnZetW6GwMOwrcSkqguLinZeiorD/eOzr1oXbzZtTvzfNmlUn8ZrWt2ix41JUFBJ+VVUY5Tp+W1kZkndFRTjeiorqUbCvuw5uvbVufz8ldKm/iorQz33y5LB89FF4fsAA2HffnT9hzZrBfvuFQcT23Tfa2BtZVVVIPOXlO95u3Vr9AU9e4kkucYmvS5Uo4glv48Ydb8vLd06OZuG1a9fC6tUhOW/fXnP8RUUhsZeXhyVTzMK2mzcPt4WF4bnEWnD8fnEx7LFH+DG4xx7VS4sW1e9BctKMx5u4bN0aXte+fRg1o0OHcL99+7D/eCKO77+ycuf3LvE93Lat+m8ZX7Zvr669J98WFlYfa+LtEUeEelHd3kcldMm0Dz4Iif2558JwA6k+ZUuXhrJf/zp897tw1lmw1167tx/3sJ299w6fkgzZuhU++ywsixbBkiUhKW7atPOybVt17So5CScm4/j9xpqLpLi4OuHFb4uLd2wqSGwu6NABOnWCjh3D0qlT6OW6fXs4zs2bq4958+aQPDt0CGXjybBjx5CUvvwyLFu2VN9WVobtJS6Jibi+zRUSKKFLND77LHSZfPxxKCsLWeWYY+DMM+HII8M0e6k+5e7w3nswfnx4/SefUHXcCcz91XhmLWhPWRm8804YFr68PNQmk5fmzXdObO4h8Xz2GXz++c67bdWquukhuQmisHDHJf4zP17jSr7fsmVIYsXF1beJNdP4NuL3469LXBLLJdf8WrTYsWlE8ocSujSI7dthxYrwU37jxp2XLVuqf/q2WzqPQ+aOY8iHj7PnpjAB9uaCtrzbYghvNzuMqZWH8XF5D0Y1e5qzfDwHVc2jkma82fJY5rccxPlr7mAB+3Aqk1na6gD69w+tPe3aVdcoE5dt23ZseogvLVqEa6169apeevYMvTiVICUbKKHLbnEPJ48WLw612cWLw7JsWajZLl8ellWr0rv4tHnzhBNVLZyDCj/iMJvG4G1v0XfTNHqtn02hh1P/VRgLug9n+j7n8Fb3M1hpe7J9O3yj+euc//QZFFkFjJ9AwUnHN+ybINJEKaHnkfXrw/nKDz8My+rVO5+cKy8PNdjkZu/KytAeumRJqPUmKiwMzd/duoUl8X6HDqENN3lp1Sok8VrbTrdsCWPQLFgAxx4b5lRNZeHC0Ltm7twwtMEVV+zc10wkxymh55j4ecK5c6uXeAJfsaK6nFlokqipPTe5Y0pBQVhXUhJ6I/boEZojevSArl2byEmtTZvCRVJPPRUmz7722vBzYdmy6ttly8IZyr33Dku3btW3vXqFs3QiWUoJPQtVVoa8tGhR9fLppzBvXkjg69dXl91zTzjwQDjggDB5Ufx2331Dgs45VVVwww1h5qZkrVuHGn5BQUjw69btuL55cxgzBn7yk9CtUiTLKKE3QatXh7GwliypXpYuDbfxNuv4FWVxXbqEjiF9+0K/fuG2b1/o3DmaY4jca69Vd2mM18KTr2LdsqW60X/ZMvjXv8KYNhUVobfNddeFSUFEsoQSeoQ2bw498OLL3LnhdnnSaDfNm4emju7dw22vXmFAxMSeGK1bR3IIuWf5crjrLvif/wndcU44Aa65JvykiV/2GG+nUhu9NDFK6I1o40Z4440wG9xrr4VxreI17ZYtq2vX/fqFppEePUIS79xZuaPRrVsHf/wj3Hnnjicf4sxCm1VNFzTttRcMHLjj0r27/pDSoJTQG9CaNSGBT5kSEvjMmaH9u6gIhgwJgxoOHQqHHAJ9+mT0YkfJlC+/hH/+M5yYiF8Cmbik+oy4hz6dZWXw8cfVz3fqBP37h7axAw+svlWilwzZVUJv1AkucsGSJWG02SlTwjJ3bni+efNwhfv114ckfvjhaiLJGi1bwqhRdX/9xo0wZ05I7mVloU3tscd2PHPdpk34Rk/VvahFi/CNf9hhYSkpqe8RSZ5SDT1NM2fCr38NTz8dHrdtG65eHzYMhg8PtfCc7FEideMersJ6/32YPz/cLlpUPShM4hI/0bJ1a3ht9+6hdvD1r1f3F00eK6B79/AlUNcxWCVrqYZeD2+/DTfdBM8+Gy6gGTs2VOb6928i/bKlaTKrvvIqnWH1tm0Lg9O89VZYpk0Ls0ztSnFxmCpw6NAdvwA+/DD0b50/v/p2xYowjs7pp8PJJ4cLFFLF8NprMGkSvPBCmLLw8svDWPj6Z88KqqHX4M03QyJ/4YUwwty118Jll+maFGlEa9aEZpv48I6Jwz0uWBCS/rRp4edjqnFuCwpCX/uDDgoJ/MUXw6+GoqKQ3E87Db7xjXCV7qRJYeTM9etDE9Sxx4bmoyVLQlPRj34UJhvv0GHHfVRUhJE333kn/NIYNWr3R9SU3aKTomlauzYM7vfww+Fz0rkz/PjH8J//qUl6pAnbvj002UybFoYyPvBAOPjgkMxbtKguV1UVyvzjH2FJPJnbuXMYVmHUqJDkW7UKyXrSJLj77nDCqFUrOPfcMCra7Nkhic+Zs+OXSbNmcNxxodxpp9XtRJK7TiDvQr0TupmdCNwFFAB/dvffJK0fA/wOiA2Azb3u/uddbbOpJPSKCnjppZDEn3oqNGP26xeuKr/wQp3YlBzlHppjXn01JOgjjth1s0pZWZiS8LHHwoekXbtwQVbiYhaGSn7ssXC+oHVr+Pa34YwzwpfB1q3VM0Rs2xa+CFas2HHYhuXLw6+IffYJ4+efdVb4QCrBf6VeCd3MCoAPgeOAJcB0YLS7z0soMwYodffL0g0q6oReXh7Gd7r33vA/1LEj/Md/hKvC4/+bIpJk7drQLNOrV80fkqqq0Jf3r3+FiRN37O2TSqdO1Vf7dusWzgPMnBm+bKqqQpNRPLnvt184R5A4kNHcufDFF+FEcXwAovjSvXv4lZJqKqGSkp2bkLJAfRP64cCN7n5C7PH1AO5+a0KZMWRJQncPPVWuvjo0Q554YqiJf/ObO/46FZEMKC8PV9fFB6Nv3nzH286dax6IfsUKeOKJ8KXw2mvhw9usWUjyUD3NYd++4Qth2bLqaahWrkwvvvhASPHla18L5xjiw0UkLjV9MRUWhi+h+BdSfOnSJXwBxn95JA5Bcd55cOWVu/9+Uv9eLt2BxQmPlwBfT1HuDDMbQajNX+3ui5MLmNlFwEUAPXv2TGPXmfXhh3DVVfD88+FL/+WXw7kfEWkgxcWhb29d7LknXHppWD7/PCT35cvD+YG+fUPyramv8JdfhgGRli7dsatofOLQ7dtDs9D774fl738PJ6GTtWlTnaD79En9q2TbtvALoaws3Ma/cBKZheOJb6tjx7q9J7XIVLfFp4Fx7r7VzC4GHgGOSS7k7g8AD0CooWdo37XatAluvjk0sRQXh9vLLlMXXpGssddeoadNulq2DGNrHHBA+q9ZtSok96qq6sTbps3uxVlZGX4dxGeA6dChuhmpsOF7iaezh6VAj4THJVSf/ATA3VcnPPwzcFv9Q8uMWbPCeZlFi+D88+E3v1GvKhFJoXPnuv+aiCsoCAkmoiSTzsgi04H9zayPmTUHzgEmJxYws24JD08F5mcuxLp79NFwNWdVFUydGnqyKJmLSK6qtYbu7hVmdhnwIqHb4oPuPtfMbgJmuPtk4AozOxWoANYAYxow5lpt3x4uBLrnHhg5MpxT6dIlyohERBpezl1Y9MUX8J3vhOsgrrkGfvvbRmm6EhFpFHkzlsu0aeEahjVrwrUN3/1u1BGJiDSenEno06eHYWv33juMwzJgQNQRiYg0rpxI6GvXhmaWrl1DLV3t5SKSj7I+obvDBReEi6+mTFEyF5H8lfUJ/c47w6Bad9wRhoMWEclXWT3D5VtvwU9/Gsbsr+OwCCIiOSNrE/qaNXD22WFwtQcf1OiIIiJZ2eRSVRUu4//88zBKZ/v2UUckIhK9rEzov/89PPNMuBK0NGX3ehGR/JN1TS5vvAHXXw9nnrl7g6+JiOS6rEvorVqFKQ///Ge1m4uIJMq6JpdBg+CFF6KOQkSk6cm6GrqIiKSmhC4ikiOU0EVEcoQSuohIjlBCFxHJEUroIiI5QgldRCRHKKGLiOSIyCaJNrOVwKI6vrwzsCqD4WSTfD12HXd+0XHXrJe7p5zKJ7KEXh9mNqOmWa9zXb4eu447v+i460ZNLiIiOUIJXUQkR2RrQn8g6gAilK/HruPOLzruOsjKNnQREdlZttbQRUQkiRK6iEiOyLqEbmYnmtkHZvaxmV0XdTwNxcweNLMVZvZewnMdzewlM/sodtshyhgbgpn1MLNXzWyemc01sytjz+f0sZtZsZm9bWazY8f969jzfcxsWuz/fYKZNY861oZgZgVm9o6ZPRN7nPPHbWYLzexdMyszsxmx5+r1f55VCd3MCoD7gJOAg4HRZnZwtFE1mIeBE5Oeuw74l7vvD/wr9jjXVADXuvvBwGHAj2J/41w/9q3AMe4+ABgInGhmhwG/Be5w9/2AtcAPoguxQV0JzE94nC/HfbS7D0zoe16v//OsSujAUOBjd1/g7tuA8cCoiGNqEO7+OrAm6elRwCOx+48ApzVmTI3B3Ze7+6zY/Y2ED3l3cvzYPdgUe1gUWxw4Bvh77PmcO24AMysBvgn8OfbYyIPjrkG9/s+zLaF3BxYnPF4Sey5fdHX35bH7nwNdowymoZlZb2AQMI08OPZYs0MZsAJ4CfgEWOfuFbEiufr/fifwU6Aq9rgT+XHcDvzTzGaa2UWx5+r1f551k0RL4O5uZjnb59TM2gBPAFe5+4ZQaQty9djdvRIYaGbtgX8AB0YbUcMzs1OAFe4+08xGRhxOYxvm7kvNbE/gJTN7P3FlXf7Ps62GvhTokfC4JPZcvvjCzLoBxG5XRBxPgzCzIkIyf8zdn4w9nRfHDuDu64BXgcOB9mYWr3jl4v/7kcCpZraQ0IR6DHAXuX/cuPvS2O0Kwhf4UOr5f55tCX06sH/sDHhz4BxgcsQxNabJwPmx++cDT0UYS4OItZ/+BZjv7n9IWJXTx25mXWI1c8ysJXAc4fzBq8CZsWI5d9zufr27l7h7b8Ln+RV3/w9y/LjNrLWZtY3fB44H3qOe/+dZd6WomZ1MaHMrAB5091uijahhmNk4YCRhOM0vgLHAJGAi0JMw9PBZ7p584jSrmdkwYArwLtVtqj8ntKPn7LGbWX/CSbACQkVrorvfZGb7EGquHYF3gO+5+9boIm04sSaXH7v7Kbl+3LHj+0fsYSHwuLvfYmadqMf/edYldBERSS3bmlxERKQGSugiIjlCCV1EJEcooYuI5AgldBGRHKGELiKSI5TQRURyxP8HagmLE1PRHLYAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = np.arange(len(valid_acc_record))\n",
        "valid_acc = torch.tensor(valid_acc_record, device='cpu')\n",
        "valid_loss = torch.tensor(valid_loss_record, device='cpu')\n",
        "\n",
        "plt.title(\"Learning curve of validation\")\n",
        "plt.plot(x, valid_acc, color=\"blue\", label=\"Valid Acc\")\n",
        "plt.plot(x, valid_loss, color=\"red\", label=\"Valid Loss\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW04.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
